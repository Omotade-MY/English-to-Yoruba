{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_union, make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv('Train.csv')\n",
    "Test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(snt):\n",
    "    return len(nltk.word_tokenize(snt))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess():\n",
    "    #English\n",
    "    global eng_vocabs, yor_vocabs\n",
    "    eng_tokenizer = Tokenizer()\n",
    "    eng_tokenizer.fit_on_texts(Train['English'])\n",
    "    dist_eng = eng_tokenizer.word_counts\n",
    "    eng_vocabs = eng_tokenizer.word_index.keys()\n",
    "\n",
    "    #Yoruba\n",
    "    yor_tokenizer = Tokenizer()\n",
    "    yor_tokenizer.fit_on_texts(Train['Yoruba'])\n",
    "    dist_yor = yor_tokenizer.word_counts\n",
    "    yor_vocabs = yor_tokenizer.word_index.keys()\n",
    "    \n",
    "    #get lenght\n",
    "    global yor_maxlen, eng_maxlen\n",
    "    yor_maxlen = Train ['Yoruba'].apply(get_len).max()\n",
    "    eng_maxlen = Train['English'].apply(get_len).max()\n",
    "    \n",
    "    #make_Xy(yor_vocabs,series_tokenize(sentences), wordy_idx, yor_len)\n",
    "    global X_train, y_train, X_test, y_test\n",
    "    x = eng_tokenizer.texts_to_sequences(Train['English'])\n",
    "    X = pad_sequences(x, maxlen = eng_maxlen, padding='post')\n",
    "    X = X.reshape((*X.shape,1))\n",
    "    X_train = X[:9000]\n",
    "    X_test = X[9000:]\n",
    "    \n",
    "    Y = yor_tokenizer.texts_to_sequences(Train['Yoruba'])\n",
    "    y = pad_sequences(Y, maxlen = yor_maxlen, padding='post')\n",
    "    y = y.reshape(*y.shape, 1)\n",
    "    y_train = y[:9000]\n",
    "    y_test = y[9000:]\n",
    "    \n",
    "   \n",
    "    #x_test = eng_tokenizer.texts_to_sequences(Train['English'].loc[9001:])\n",
    "    #Y_test = yor_tokenizer.texts_to_sequences(Train['Yoruba'].loc[9001:])\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 200, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 243, 1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape\n",
    "# Hyperparameters\n",
    "learning_rate = 0.005\n",
    "\n",
    "# TODO: Build the layers\n",
    "model = Sequential()\n",
    "model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(yor_maxlen, activation='softmax'))) \n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_3 (GRU)                  (None, 200, 256)          198144    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 200, 1024)         263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 1024)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 200, 243)          249075    \n",
      "=================================================================\n",
      "Total params: 710,387\n",
      "Trainable params: 710,387\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 1800 samples\n",
      "WARNING:tensorflow:From C:\\Users\\MUHAMMAD\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=1024, epochs = 20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
